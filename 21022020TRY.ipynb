{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "21022020TRY.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAOHgZ7eeX/2Vl4rGTMdv6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashokmuruga/AIMLDL/blob/master/21022020TRY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgH2DWsmz5S4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "14f2ccf2-6a6e-4511-e9e3-7c5db021d041"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#classifier = load_model('./emotion_detector_models/model_v3_71.hdf5')\n",
        "model = load_model('/content/model.h5')\n",
        "predicted_class = np.argmax(model.predict('face.jpeg'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-43b6559fea11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#classifier = load_model('./emotion_detector_models/model_v3_71.hdf5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'face.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot create group in read-only mode.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot create group in read-only mode."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLf2uBi96Hqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "62caae5e-e5eb-409b-f88f-9f4fa0377508"
      },
      "source": [
        "!curl -O https://s3.amazonaws.com/model-server/inputs/ferplus-input.jpg"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  105k  100  105k    0     0  18746      0  0:00:05  0:00:05 --:--:-- 33282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fb79UmT6RJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -X POST http://127.0.0.1:8080/predictions/FERPlus -T ferplus-input.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN6jKVSr6uEv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "299801dc-f8e5-467f-a514-cf86454f81a6"
      },
      "source": [
        "!curl -X PUT 'http://localhost:8080/services/age_real' -d \n",
        "'{\n",
        " \"description\": \"age estimation service\",\n",
        " \"model\": {\n",
        "  \"repository\": \"/opt/models/age_real\",\n",
        "  \"init\": \"https://deepdetect.com/models/init/desktop/images/classification/age_real.tar.gz\",\n",
        "  \"create_repository\":true\n",
        " },\n",
        " \"mllib\": \"caffe\",\n",
        " \"type\": \"supervised\",\n",
        " \"parameters\": {\n",
        "  \"input\": {\n",
        "   \"connector\": \"image\"\n",
        "  }\n",
        " }\n",
        "}'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-615336bca255>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \"description\": \"age estimation service\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYxbb2f47i81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -X POST http://127.0.0.1:8080/predictions/FERPlus -T face.jpeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5stbYwG9kwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "e90225fe-def6-41eb-d49f-c05dcf6a61c4"
      },
      "source": [
        "!curl -X PUT 'http://localhost:8080/services/ilsvrc_googlenet' -d '{\n",
        "  \"description\": \"image classification service\",\n",
        "  \"model\": {\n",
        "  \"repository\": \"/opt/models/ilsvrc_googlenet\",\n",
        "  \"init\": \"https://deepdetect.com/models/init/desktop/images/classification/ilsvrc_googlenet.tar.gz\",\n",
        "  \"create_repository\": true\n",
        "            },\n",
        " \"mllib\": \"caffe\",\n",
        " \"type\": \"supervised\",\n",
        " \"parameters\":\n",
        "  {\n",
        "  \"input\": \n",
        "   {\n",
        "   \"connector\": \"image\"\n",
        "   }\n",
        "  }\n",
        " }'\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-d66e41b10c99>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \"description\": \"image classification service\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoGXdS1L-exm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "00988a37-679e-4079-cb5c-980846f3c46c"
      },
      "source": [
        "curl -X POST 'http://localhost:8080/predict' -d '{\n",
        "  \"service\": \"ilsvrc_googlenet\",\n",
        "  \"parameters\": {\n",
        "    \"output\": {\n",
        "      \"best\": 3\n",
        "    },\n",
        "    \"mllib\": {\n",
        "      \"gpu\": true\n",
        "    }\n",
        "  },\n",
        "  \"data\": [\n",
        "    \"/data/example.jpg\"\n",
        "  ]\n",
        "}'\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-29fcd2964458>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    curl -X POST 'http://localhost:8080/predict' -d '{\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp_9ORebCzNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "215628f7-6b22-4f25-c23c-72a06f7562cf"
      },
      "source": [
        "!python emotions.py --mode display"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  File \"emotions.py\", line 8\n",
            "    <html lang=\"en\">\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l12mvMkD0Zl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "7f5861aa-fae7-40ca-88a5-8b7de588d155"
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# command line argument\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"--mode\",help=\"train/display\")\n",
        "a = ap.parse_args()\n",
        "mode = a.mode \n",
        "\n",
        "def plot_model_history(model_history):\n",
        "    \"\"\"\n",
        "    Plot Accuracy and Loss curves given the model_history\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    fig.savefig('plot.png')\n",
        "    plt.show()\n",
        "\n",
        "# Define data generators\n",
        "train_dir = 'data/train'\n",
        "val_dir = 'data/test'\n",
        "\n",
        "num_train = 28709\n",
        "num_val = 7178\n",
        "batch_size = 64\n",
        "num_epoch = 50\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# If you want to train the same model or try other models, go for this\n",
        "if mode == \"train\":\n",
        "    model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "\n",
        "    model_info = model.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch=num_train // batch_size,\n",
        "            epochs=num_epoch,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=num_val // batch_size)\n",
        "\n",
        "    plot_model_history(model_info)\n",
        "    model.save_weights('model.h5')\n",
        "\n",
        "# emotions will be displayed on your face from the webcam feed\n",
        "elif mode == \"display\":\n",
        "    model.load_weights('model.h5')\n",
        "\n",
        "    # prevents openCL usage and unnecessary logging messages\n",
        "    cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "    # dictionary which assigns each label an emotion (alphabetical order)\n",
        "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "    # start the webcam feed\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while True:\n",
        "        # Find haar cascade to draw bounding box around face\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "            cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
        "            roi_gray = gray[y:y + h, x:x + w]\n",
        "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
        "            prediction = model.predict(cropped_img)\n",
        "            maxindex = int(np.argmax(prediction))\n",
        "            cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--mode MODE]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-224bc43d-3f97-4edd-9788-24afc06ade3d.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O_vvNt7FE3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "84721df5-3404-44d0-a335-54841c80d0c6",
        "id": "2abUAxbyFFZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "!python one.py --mode display"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"one.py\", line 60, in <module>\n",
            "    class_mode='categorical')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 540, in flow_from_directory\n",
            "    interpolation=interpolation\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\", line 106, in __init__\n",
            "    for subdir in sorted(os.listdir(directory)):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/train'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}